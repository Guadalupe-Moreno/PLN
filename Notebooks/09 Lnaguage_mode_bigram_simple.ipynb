{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimación de un modelo del lenguaje (bigramas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para crear un modelo del lenguaje $\\mu=(\\Sigma,A,\\Pi)$, indexaremos las palabras del corpus de entrenamiento a partir de número. En el vocabulario guardaremos la palabra y el índice numérico respectivo. Para esto, definimos las siguientes dos funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- encoding:utf-8 -*-\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from preprocessing import preprocess\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "\n",
    "#Funcion que crea un vocabulario de palabras con un indice numerico\n",
    "def vocab():\n",
    "    vocab = defaultdict()\n",
    "    vocab.default_factory = lambda: len(vocab)\n",
    "    return vocab    \n",
    "\n",
    "#Funcion que pasa la cadena de simbolos a una secuencia con indices numericos\n",
    "def text2numba(corpus, vocab):\n",
    "    for doc in corpus:\n",
    "        yield [vocab[w] for w in doc.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento de los datos\n",
    "\n",
    "Para el modelo del lenguaje, asumiremos un corpus de entrenamiento muy simple. En este caso, creamos el vocabulario 'idx' e indexamos cada símbolo del vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3, 4], [3, 5, 6], [0, 5, 7, 8, 9], [3, 10, 2, 11]]\n"
     ]
    }
   ],
   "source": [
    "corpus = ['el perro come un hueso', 'un muchacho jugaba', 'el muchacho saltaba la cuerda',\n",
    "          'un gato come croquetas']\n",
    "\n",
    "#Llamamos la funcion para crear el vocabulario\n",
    "idx = vocab()\n",
    "#Creamos el vocabulario y le asignamos un indice a cada simbolo segun su aparicion\n",
    "cads_idx = list(text2numba(corpus,idx))\n",
    "\n",
    "print(cads_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que el vocabulario es un diccionario donde se asocian los símbolos y sus índices numéricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function vocab.<locals>.<lambda> at 0x0000027568211F28>, {'el': 0, 'perro': 1, 'come': 2, 'un': 3, 'hueso': 4, 'muchacho': 5, 'jugaba': 6, 'saltaba': 7, 'la': 8, 'cuerda': 9, 'gato': 10, 'croquetas': 11})\n"
     ]
    }
   ],
   "source": [
    "print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora debemos indicar el inicio y el fin de cada cadena. Para esto, utilizaremos las estiquetas BOS (Beginning Of Sentence) y EOS (End Of Sentence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13, 0, 1, 2, 3, 4, 12], [13, 3, 5, 6, 12], [13, 0, 5, 7, 8, 9, 12], [13, 3, 10, 2, 11, 12]]\n"
     ]
    }
   ],
   "source": [
    "#Indicamos las etiquetas a usar\n",
    "EOS = '<EOS>'\n",
    "BOS = '<BOS>'\n",
    "\n",
    "#Cada etiqeuta se le asigna un indice numerico\n",
    "BOS_IDX = max(idx.values())+2\n",
    "EOS_IDX = max(idx.values())+1\n",
    "\n",
    "#Se agregan estas etiqeutas al vocabulario\n",
    "idx[EOS] = EOS_IDX\n",
    "idx[BOS] = BOS_IDX\n",
    "\n",
    "#A cada cadena se le agrega la etiqueta BOS al inicio y EOS al final\n",
    "cadenas = [[BOS_IDX] + cad + [EOS_IDX] for cad in cads_idx]\n",
    "\n",
    "print(cadenas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos los bigramas a partir de las cadenas de entrenamiento. En este caso, incluimos bigramas del tipo ($<BOS>$,$w_0$) y ($w_T$,$<EOS>$) que nos serán útiles para construir el modelo. Asimismo, obtenemos las frecuencias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(13, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 12), (13, 3), (3, 5), (5, 6), (6, 12), (13, 0), (0, 5), (5, 7), (7, 8), (8, 9), (9, 12), (13, 3), (3, 10), (10, 2), (2, 11), (11, 12)]\n"
     ]
    }
   ],
   "source": [
    "#Se crean los bigramas\n",
    "bigrams = list(chain(*[zip(cad,cad[1:]) for cad in cadenas]))\n",
    "\n",
    "#Se obtiene la frecuencia de cada bigrama\n",
    "frecBigrams = Counter(bigrams)\n",
    "\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimación del modelo\n",
    "\n",
    "Obtenemos el número de elementos que hay en el vocabulario y creamos la matriz de transiciones $A$ y el vector de probabilidades iniciales $\\Pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numero de tipos sin contar las etiqeutas BOS y EOS\n",
    "N = len(idx)-2\n",
    "\n",
    "#La matriz A es de NxN+1, los renglones consideran EOS\n",
    "A = np.zeros((N,N+1))\n",
    "\n",
    "#Pi es de tamano N\n",
    "Pi = np.zeros(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedemos a llenar la matriz $A$ y el vector $\\Pi$ con las frecuencias de bigramas. Para la matriz de transiciones, tenemos que:\n",
    "\n",
    "$$A = (a_{ij}) = fr([w_i,w_j])$$\n",
    "\n",
    "Se considera a la etiqueta EOS en los finales de cadenas. Para el vector de iniciales tenemos que:\n",
    "\n",
    "$$\\Pi = (\\pi_i) = fr([<BOS>,w_i])$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bigram,frec in frecBigrams.items():\n",
    "    if bigram[0] != BOS_IDX:\n",
    "        A[bigram[0],bigram[1]] = frec\n",
    "    elif bigram[0] == BOS_IDX:\n",
    "        Pi[bigram[1]] = frec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, el modelo sólo cuenta con frecuencias. Falta obtener las probabilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1.]]\n",
      "[2. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Frecuencias de bigramas\n",
    "print(A.T)\n",
    "\n",
    "print(Pi.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos un estimador Lidstone, que definiremos como:\n",
    "\n",
    "$$p_{lid}(w_j|w_i) = \\frac{fr([w_i,w_j]) + \\lambda}{\\sum_k fr([w_i,w_k]) + \\lambda N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definicion de estimador Lidstone\n",
    "def get_model_Lid(frec_trans, frec_init, l=0.0):\n",
    "    #Añadir parámetro\n",
    "    addA = frec_trans + l\n",
    "    addPi = frec_init + l\n",
    "    \n",
    "    #Obtener probabilidades de transición\n",
    "    mu_A = addA.T/addA.T.sum(0)\n",
    "    #Obtener probabilidades de inicio\n",
    "    mu_Pi = addPi.T/addPi.T.sum(0)\n",
    "    \n",
    "    return (mu_A.T,mu_Pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definida la función así, podemos estimar la probabilidad de los datos a partir de variar el parámetro $\\lambda$. En primer lugar, podemos obtener la probabilidad frecuentista (que llamaremos MLE) dada por:\n",
    "\n",
    "$$p_{mle}(w_j|w_i) = \\frac{fr([w_i,w_j])}{\\sum_k fr([w_i,w_k])}$$\n",
    "\n",
    "En este caso, $\\lambda =0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.5  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.  ]\n",
      " [0.   0.   0.5  0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.33 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.5  0.   0.   0.33 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.5  0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.5  0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.33 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.5  0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   1.   0.   1.   0.   0.   1.   0.   1.  ]]\n",
      "[0.5 0.  0.  0.5 0.  0.  0.  0.  0.  0.  0.  0. ]\n"
     ]
    }
   ],
   "source": [
    "#Se obtienen las probabilidades\n",
    "A_mle, Pi_mle = get_model_Lid(A,Pi,l=0)\n",
    "\n",
    "print(np.round(A_mle,2).T)\n",
    "print(Pi_mle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos comprobar que nuestro estimador es una probabilidad, para esto vemos que la matriz de transiciones cumple:\n",
    "\n",
    "$$\\sum_{j=1}^{N+1} (a_{ij}) = 1$$\n",
    "\n",
    "(En este caso, hemos tomado en cuenta al símbolo de término EOS). Y el vector de probabilidades iniciales cumple:\n",
    "\n",
    "$$\\sum_{i=1}^{N} \\pi_i = 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(A_mle.sum(1))\n",
    "print(Pi_mle.sum(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos nuestro modelo $\\mu$, podemos obtener la probabilidad de cadenas. Para esto definimos una función dada por:\n",
    "\n",
    "$$p(w_0 ... w_T) = \\pi_0 \\prod_{i=1}^T p(w_i|w_{i-1})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_cad(cad,mu):\n",
    "    #Matrices del modelo\n",
    "    A_mu, Pi_mu = mu\n",
    "    #Obtenemos los simbolos\n",
    "    seq = cad.split()\n",
    "    #Obtenemos los bigramas de la cadena de evaluacion\n",
    "    bigrSeq = zip(seq,seq[1:])\n",
    "    \n",
    "    #Guardamos la probabilidad inicial dado el modelo\n",
    "    p = Pi_mu[idx[seq[0]]]\n",
    "    #Multiplicamos por las probabilidades de los bigramas dado el modelo\n",
    "    for gram1,gram2 in bigrSeq:\n",
    "        p *= A_mu[idx[gram1],idx[gram2]]\n",
    "    \n",
    "    return p  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos, entonces, probar como es que este modelo del lenguaje estima las probabilidades de dos cadenas: una contenida en los datos; y otra agramatical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t--Probabilidad MLE--\n",
      "el perro come un hueso : \t 0.041666666666666664\n",
      "un perro come un hueso : \t 0.0\n"
     ]
    }
   ],
   "source": [
    "#Cadenas de prueba\n",
    "str1 = 'el perro come un hueso'\n",
    "str2 = 'un perro come un hueso'\n",
    "\n",
    "print('\\t--Probabilidad MLE--')\n",
    "print( str1,': \\t', prob_cad(str1,(A_mle,Pi_mle)) )\n",
    "print( str2,': \\t', prob_cad(str2,(A_mle,Pi_mle)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De igual forma, podemos utilizar una probabilidad laplaciana para definir el modelo. En este caso, utilizaremos la función de probabilidad dada por:\n",
    "\n",
    "\n",
    "$$p_{lap}(w_j|w_i) = \\frac{fr([w_i,w_j]) + 1}{\\sum_k fr([w_i,w_k]) + N}$$\n",
    "\n",
    "Es decir, tomaremos $\\lambda = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "1.0\n",
      "[[0.06666667 0.13333333 0.06666667 0.06666667 0.06666667 0.13333333\n",
      "  0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      "  0.06666667]\n",
      " [0.07142857 0.07142857 0.14285714 0.07142857 0.07142857 0.07142857\n",
      "  0.07142857 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857\n",
      "  0.07142857]\n",
      " [0.06666667 0.06666667 0.06666667 0.13333333 0.06666667 0.06666667\n",
      "  0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.13333333\n",
      "  0.06666667]\n",
      " [0.0625     0.0625     0.0625     0.0625     0.125      0.125\n",
      "  0.0625     0.0625     0.0625     0.0625     0.125      0.0625\n",
      "  0.0625    ]\n",
      " [0.07142857 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857\n",
      "  0.07142857 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857\n",
      "  0.14285714]\n",
      " [0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      "  0.13333333 0.13333333 0.06666667 0.06666667 0.06666667 0.06666667\n",
      "  0.06666667]\n",
      " [0.07142857 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857\n",
      "  0.07142857 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857\n",
      "  0.14285714]\n",
      " [0.07142857 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857\n",
      "  0.07142857 0.07142857 0.14285714 0.07142857 0.07142857 0.07142857\n",
      "  0.07142857]\n",
      " [0.07142857 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857\n",
      "  0.07142857 0.07142857 0.07142857 0.14285714 0.07142857 0.07142857\n",
      "  0.07142857]\n",
      " [0.07142857 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857\n",
      "  0.07142857 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857\n",
      "  0.14285714]\n",
      " [0.07142857 0.07142857 0.14285714 0.07142857 0.07142857 0.07142857\n",
      "  0.07142857 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857\n",
      "  0.07142857]\n",
      " [0.07142857 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857\n",
      "  0.07142857 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857\n",
      "  0.14285714]]\n"
     ]
    }
   ],
   "source": [
    "#Modelo con probabilidad laplaciana\n",
    "A_lap, Pi_lap = get_model_Lid(A,Pi,l=1)\n",
    "\n",
    "#Comprobar que suman 1\n",
    "print(A_lap.sum(1))\n",
    "print(Pi_lap.sum(0))\n",
    "print(A_lap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y podemos comparar las probabilidades que se obtienen con este estimador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t--Probabilidad Laplaciana--\n",
      "el perro come un hueso : \t 5.9523809523809524e-05\n",
      "un perro come un hueso : \t 2.7901785714285713e-05\n"
     ]
    }
   ],
   "source": [
    "print('\\t--Probabilidad Laplaciana--')\n",
    "print( str1,': \\t', prob_cad(str1,(A_lap,Pi_lap)) )\n",
    "print( str2,': \\t', prob_cad(str2,(A_lap,Pi_lap)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puede observarse, al tomar la probabilidad Laplaciana, las transiciones que no se vieron en el corpus tienen una probabilidad diferente de 0, pero que es baja. Esto es una ventaja, pues podemos asignar probabilidades a transiciones no atestiguadas, asignándoles baja probabilidad de transición."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generación de cadenas\n",
    "\n",
    "Una vez establecido el modelo del lenguaje, éste puede utilizarse en diferentes aplicaciones de NLP. Los modelos del lenguaje, en general están presentes en un gran número de tareas (traducción automática, reconocimiento de voz, reconocimiento óptico de caracteres, etc.). Una de sus aplicaciones más inmediatas es el autocompletado. Esto es, predecir la palabra $\\hat{w}$ más probable dada una cadena $w_1,...,w_{i-1}$ de palabras anteriores. Para determinar esta predicción, buscamos:\n",
    "\n",
    "$$\\hat{w} = \\arg\\max_{w} p(w_1 w_2...w_{n-1} w)$$\n",
    "\n",
    "Esta probabilidad se puede determinar a partir del modelo $\\mu$ que hemos definido de la forma siguiente:\n",
    "\n",
    "$$\\hat{w} = \\arg\\max_{w} p(w|w_{n-1})p(w_1 w_2...w_{n-1})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabra siguiente más probable: come Ccon probabilidad: 0.14285714285714285\n"
     ]
    }
   ],
   "source": [
    "#Diccionario para recuperar palabras a partir de sus índices\n",
    "get_word = {i:w for w,i in idx.items()}\n",
    "\n",
    "def next_word(string, mu):\n",
    "    #Elementos del modelo\n",
    "    A, Pi = mu\n",
    "    #Obtener la probabilidad de la historia\n",
    "    p_prev = prob_cad(string, mu)\n",
    "    #Obtener la última palabra en la historia\n",
    "    last_w = string.split()[-1]\n",
    "    #Obtener el argumento que maximiza la probabilidad\n",
    "    max_w = np.argmax(A[idx[last_w]]*p_prev)\n",
    "    \n",
    "    return get_word[max_w], A[idx[last_w],max_w]\n",
    "    \n",
    "pred, prob_pred = next_word('el perro',(A_lap,Pi_lap))\n",
    "print('Palabra siguiente más probable:', pred, 'Ccon probabilidad:', prob_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si aplicamos esta predicción d eforma iterativa, podemos, además, producir texto a partir de una cadena de entrada. Para esto, buscaremos encontrar la cadena con mayor probabilidad dado una estimulo de entrada. Este proceso acabará cuando la palabra siguiente con mayor probabilidad sea EOS (End Of String)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el perro come un hueso\n"
     ]
    }
   ],
   "source": [
    "def generate(string, mu):\n",
    "    #Guarda la palabra predicha\n",
    "    w = ''\n",
    "    #Guarda la cadena que se ha generado\n",
    "    str_gen = string\n",
    "    #El método se detiene al ver <EOS>\n",
    "    while w != '<EOS>':\n",
    "        #Predice la siguiente palabra\n",
    "        w, p = next_word(str_gen, mu)\n",
    "        #Agrega esa palabra a ala cadena\n",
    "        str_gen += ' ' + w\n",
    "    \n",
    "    #Regresa la cadena si el símbolo EOS\n",
    "    return str_gen[:len(str_gen)-6]\n",
    "    \n",
    "print(generate('el', (A_lap,Pi_lap) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación del modelo del lenguaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para tener una idea de que tan bueno es el modelo que estimamos, podemos utilizar la entropía cruzada empírica dada por la función:\n",
    "\n",
    "$$H_E(p) = -\\frac{1}{M} \\sum_{x_1,...,x_m}^N \\log p(x_1, ...x_m)$$\n",
    "\n",
    "En este caso, el modelo con menor entropía será aquel que mejor prediga una (o varias) cadenas no vistas. Por tanto, consideraremos que generaliza mejor. De aquí, que busquemos un modelo que minimice esta función.\n",
    "\n",
    "En otros casos, se utiliza la perplejidad, la cual se define en base a esta entropía como $2^{H_E(p)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corpus para evaluar el modelo (cadenas no vistas)\n",
    "eval_corp = ['el perro jugaba un hueso','el muchacho jugaba la cuerda', 'un muchacho come un hueso']\n",
    "\n",
    "def H(mu):\n",
    "    #Entropía\n",
    "    H = 0.0\n",
    "    for cad in eval_corp:\n",
    "        #Probabilidad de la cadena\n",
    "        p_cad = prob_cad(cad,mu)\n",
    "        #Número de bigramas\n",
    "        M = len(cad.split())\n",
    "        #Obtenemos la entropía cruzada de la cadena\n",
    "        H -= np.log(p_cad)/M       \n",
    "    \n",
    "    return H/len(eval_corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropía, modelo MLE: \t inf\n",
      "Entropía, modelo Laplaciano\t: 2.126066556461792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mijangos\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "print( 'Entropía, modelo MLE: \\t', H((A_mle,Pi_mle)) )\n",
    "print( 'Entropía, modelo Laplaciano\\t:', H((A_lap,Pi_lap)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo de frecuentista (MLE) es incapaz de asignarle una probabilidad distinta de 0 a cadenas que nunca vio; por tanto, no es capaz de generalizar adecuadamente. En este sentido, la probabilidad con smoothing laplaciano resulta ser un mejor estimador.\n",
    "\n",
    "Pero ¿cuál sería el mejor estimador que podríamos obtener? Si nos enfocamos a la familia de estimadores de Lidstone, basta variar el parámetro $\\lambda$ y determinar cuándo se minimiza la entropía cruzada. Para esto, tomamos varios valores de $\\lambda$ (desde 0.01 hasta 2) y observamos cómo se comporta la entropía cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Guarda valores de entropía\n",
    "rank_H = []\n",
    "for i in range(1,200):\n",
    "    #rango de 0.01 a 2\n",
    "    k = float(i)/100\n",
    "    #obtiene los valores de entropia\n",
    "    rank_H.append( (k,H(get_model_Lid(A,Pi,l=k))) )\n",
    "\n",
    "#Visualización\n",
    "rank_H = np.array(rank_H)\n",
    "plt.scatter(rank_H[:,0],rank_H[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así, podemos obtener el valor de $\\lambda$ que minimice la entropía y estimar una distribución de probabilidad Lidstone con este parámetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07\n"
     ]
    }
   ],
   "source": [
    "#Parámetro que minimiza entropía\n",
    "l_min = rank_H[:,0][np.argmin(rank_H[:,1])]\n",
    "print(l_min)\n",
    "\n",
    "#Estimación de probabilidad\n",
    "A_lid, Pi_lid = get_model_Lid(A,Pi,l=l_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y al igual que hicimos con los otros estimadores, podemos determinar la probabilidad de cadenas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t--Probabilidad Lidstone--\n",
      "el perro come un hueso : \t 0.008864698962808098\n",
      "un perro come un hueso : \t 0.0004316129690761788\n"
     ]
    }
   ],
   "source": [
    "print('\\t--Probabilidad Lidstone--')\n",
    "print( str1,': \\t', prob_cad(str1,(A_lid,Pi_lid)) )\n",
    "print( str2, ': \\t', prob_cad(str2,(A_lid,Pi_lid)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, es posible combinar funciones de probabilidad a partir de la interpolación, dada por:\n",
    "\n",
    "$$p_{int}(w_j|w_i) = \\sum_i \\lambda_i \\cdot p_i(w_j|w_i)$$\n",
    "\n",
    "Recordando que $\\sum_i \\lambda_i =1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t--Probabilidad Interpolación\n",
      "el perro come un hueso : \t 0.018492254641849337\n",
      "perro hueso el jugaba un : \t 8.77158126400316e-10\n",
      "\n",
      "Entropía de la interpolación: 1.7379299563197428\n"
     ]
    }
   ],
   "source": [
    "#Interpolación\n",
    "l1, l2, l3 = 0.3, 0.1, 0.6\n",
    "A_int = l1*A_lid + l2*A_lap + l3*A_mle\n",
    "Pi_int = l1*Pi_lid + l2*Pi_lap + l3*Pi_mle\n",
    "\n",
    "print('\\t--Probabilidad Interpolación')\n",
    "print( str1,': \\t', prob_cad(str1,(A_int, Pi_int)) )\n",
    "print( str2, ': \\t', prob_cad(str2,(A_int, Pi_int)) )\n",
    "\n",
    "print( '\\nEntropía de la interpolación:', H((A_int, Pi_int)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
