{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I_T8sYgETX3L"
   },
   "source": [
    "# Modelos del lenguaje a partir de n-gramas\n",
    "\n",
    "La estimación de modelos del lenguaje busca encontrar la probabilidad de una palabra dada su historia. En el caso de los bigramas, se asume la propiedad de Markov y se toma únicamente el elemento inmediatamente anterior. En el caso de modelos de n-gramas, con $n>2$, la palabra depende de más elementos anteriores.\n",
    "Así, en el modelo de trigramas, se toma en cuenta las dos palabras anteriores. En el caso de 4-gramas, se toma en cuenta los 3 elementos anteriores. Y en general, para un modelo de n-gramas, se toma en cuenta los $n-1$ elementos anteriores. El modelo del lenguaje, entonces, se define como:\n",
    "\n",
    "$$\\mu = (\\Sigma, A, \\Pi)$$\n",
    "\n",
    "donde $\\Sigma$ es el vocabulario, $A$ es un tensor que guarda las probabilidades de transiciones y $\\Pi$ guarda las probabilidades iniciales.\n",
    "\n",
    "A continuación, estimamos modelos del lenguaje para un corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bV9ZtWeS_rMP"
   },
   "outputs": [],
   "source": [
    "#-*- encoding:utf-8 -*-\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "from re import sub\n",
    "\n",
    "#Funcion que crea un vocabulario de palabras con un indice numerico\n",
    "def vocab():\n",
    "    vocab = defaultdict()\n",
    "    vocab.default_factory = lambda: len(vocab)\n",
    "    return vocab    \n",
    "\n",
    "#Funcion que pasa la cadena de simbolos a una secuencia con indices numericos\n",
    "def text2numba(corpus, vocab):\n",
    "    for doc in corpus:\n",
    "        yield [vocab[w] for w in doc.strip().split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XJ1cbSJhUpmg"
   },
   "source": [
    "### Preprocesamiento del corpus\n",
    "\n",
    "Para estimar las probabilidades, necesitamos un corpus. Procesaremos a preprocesarlo. En este caso, eliminamos signos de puntuaciónn y caracteres no alfanuméricos. Se debe señalar, que en muchos casos, se conservan los signos de puntuación como elementos pertenencientes al vocabulario. Esto con el objetivo de generar lenguaje escrito que preserve estos signos. Dado que esto no es ahora nuestra principal preocupación, eliminamos estos signos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "CSzDt5wx_rMn",
    "outputId": "f563f903-ca45-41c0-802e-fb0960a94834"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comparación con otros modelos', 'como hemos señalado anteriormente los modelos implicacionales de wp muestran ventajas metodológicas al tomar a la palabra como unidad mínima de análisis', ' la asignación de rasgos morfosintácticos sólo se da en la palabra completa ninguna unidad formal menor a la palabra contiene rasgos ni características morfosintácticas', ' esta perspectiva evita una búsqueda exhaustiva de patrones segmentales que no necesariamente forman parte de la realidad lingüística', ' de esta forma el marco implicacional deja de lado discusiones teóricas como la pertinencia de la vocal temática o las variaciones de tema verbal', ' ', '\\tacerca la flexión verbal del español los enfoques clásicos han dado gran peso a una discusión sobre la precisión de diferentes lindes segmentales y la asignación pertinente de rasgos esto es la estructura verbal de la palabra', ' el marco implicacional propuesto tiene poco interés en la estructura interna de la palabra y se concentra más en el análisis de la estructuración de los paradigmas a partir de las palabras su estructura formal es un epifenómeno consecuencia de procesos abstractivos', ' consideramos que esta perspectiva a pesar de haber sido poco abordada en la literatura muestra ventajas ante otras perspectivas', ' sin embargo debemos considerar también las desventajas del análisis para proponer nuevas alternativas dentro de los modelos implicacionales']\n"
     ]
    }
   ],
   "source": [
    "#Se abre el corpus, se limpia y se obtiene cada una de las líneas en éste\n",
    "corpus = sub(r'[^\\w\\s]','', '\\n'.join(open('corpus.es.txt','r', encoding='utf8').read().lower().strip().split('.')) ).split('\\n')\n",
    "\n",
    "#Se remueven las cadenas vacías\n",
    "while(\"\" in corpus): \n",
    "    corpus.remove(\"\")\n",
    "    \n",
    "print(corpus[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UjhYNYXGVTse"
   },
   "source": [
    "En primer lugar, tomaremos una parte del corpus y obtendremos los corpus de entrenamiento y de evaluación. En este caso, tomaremos el $70\\%$ de los datos para estimar el modelo, mientras que el $30\\%$ restante será parte de la evaluación, es decir, se utilizará para calcular la entropía (o, en otros casos, la perplejidad)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "qqGu_Bj0COyS",
    "outputId": "2de3c025-f1a2-418d-d29a-44879db73bb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "corpus = corpus[:100]\n",
    "\n",
    "#Se obtiene el corpus de entrenamiento y evaluación\n",
    "train_data, test_data = train_test_split(corpus, test_size=0.3)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RvNGzOiPV20A"
   },
   "source": [
    "A continuación, nos encargamos de pasar las cadenas de palabras a cadenas de índices numéricos. En este sentido, utilizaremos enteros para estimar el modelo. Crearemos dos diccionarios: uno que va toma una palabra y lo convierte en un índice (este nos servirá para acceder a las probabilidades en el modelo) y otro que tome los índices y los lleve a la palabra (este diccionario nos ayudará a recuperar los datos textuales a partir de los índices en el modelo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "WHMCcahJ_rNR",
    "outputId": "24c5275d-a769-4ea2-8d1b-56044fca6cf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1, 17, 18, 19, 20, 21, 22, 1, 23, 24, 18, 25, 26, 27, 1, 28, 3, 18, 1, 29], [30, 31, 32, 33, 34, 35, 36, 37, 1, 28, 38, 18, 1, 29, 22, 39, 40, 41, 37, 30, 42, 18, 1, 43, 18, 6, 44, 13, 45, 18, 46, 47, 48, 28, 49, 27, 50, 51, 52, 18, 53, 54], [55]]\n",
      "defaultdict(<function vocab.<locals>.<lambda> at 0x7f5a566a4af0>, {'acerca': 0, 'la': 1, 'flexión': 2, 'verbal': 3, 'del': 4, 'español': 5, 'los': 6, 'enfoques': 7, 'clásicos': 8, 'han': 9, 'dado': 10, 'gran': 11, 'peso': 12, 'a': 13, 'una': 14, 'discusión': 15, 'sobre': 16, 'precisión': 17, 'de': 18, 'diferentes': 19, 'lindes': 20, 'segmentales': 21, 'y': 22, 'asignación': 23, 'pertinente': 24, 'rasgos': 25, 'esto': 26, 'es': 27, 'estructura': 28, 'palabra': 29, 'el': 30, 'marco': 31, 'implicacional': 32, 'propuesto': 33, 'tiene': 34, 'poco': 35, 'interés': 36, 'en': 37, 'interna': 38, 'se': 39, 'concentra': 40, 'más': 41, 'análisis': 42, 'estructuración': 43, 'paradigmas': 44, 'partir': 45, 'las': 46, 'palabras': 47, 'su': 48, 'formal': 49, 'un': 50, 'epifenómeno': 51, 'consecuencia': 52, 'procesos': 53, 'abstractivos': 54, 'amaban': 55, '2': 56, 'modelos': 57, 'basados': 58, 'ia': 59, 'basevtpsd': 60, 'así': 61, 'propuesta': 62, 'basada': 63, 'combinación': 64, 'ta': 65, 'np': 66, 'puede': 67, 'ser': 68, 'siguiente': 69, 'que': 70, 'retomamos': 71, 'stockwell': 72, 'bowen': 73, 'martin': 74, '1965': 75, '105': 76, '1a': 77, 'mientras': 78, 'otros': 79, 'casos': 80, 'le': 81, 'ha': 82, 'atribuído': 83, 'únicamente': 84, 'categorías': 85, 'este': 86, 'morfema': 87, 'dejando': 88, 'lado': 89, 'modo': 90, 'o': 91, 'bien': 92, 'atribuyéndose': 93, 'vocal': 94, 'temática': 95, 'ejemplo': 96, '3': 97, 'dentro': 98, 'comúnmente': 99, 'reconocen': 100, 'morfemas': 101, 'harris': 102, '1969': 103, '77': 104, 'propone': 105, 'son': 106, 's': 107, 'morfo': 108, 'cero': 109, 'mos': 110, 'por': 111, 'otro': 112, 'toma': 113, 'cuenta': 114, 'tam': 115, 'definida': 116, 'forma': 117, '1988': 118, '1b': 119, 'sin': 120, 'embargo': 121, 'concluir': 122, 'estudios': 123, 'actuales': 124, 'categorìas': 125, 'como': 126, 'señala': 127, 'pons': 128, '1966': 129, '74': 130, 'problema': 131, 'fundamental': 132, 'ciertamente': 133, 'criterio': 134, 'seguido': 135, 'para': 136, 'identificación': 137, 'autores': 138, '54': 139, 'sugerido': 140, 'muestra': 141, 'tiempo': 142, 'aspecto': 143, 'lo': 144, 'deja': 145, 'al': 146, 'aparte': 147, 'categoría': 148, 'no': 149, 'siempre': 150, 'sido': 151, 'reconocida': 152, 'discusiones': 153, 'organización': 154, 'morfosintácticas': 155, 'cobran': 156, 'especial': 157, 'pues': 158, 'busca': 159, 'proponer': 160, 'preferentemente': 161, 'concatenativa': 162, '24': 163, '1': 164, 'con': 165, 'respecto': 166, 'tema': 167, 'existen': 168, 'conflictos': 169, 'teóricos': 170, 'pertinencia': 171, 'vt': 172, 'algunos': 173, 'hemos': 174, 'señalado': 175, 'consideran': 176, 'modelo': 177, 'volveremos': 178, 'adelante': 179, 'variaciones': 180, 'estas': 181, 'propuestas': 182, 'base': 183, 'nos': 184, 'permitirá': 185, 'realizar': 186, 'comparación': 187, 'amplia': 188, 'suscitado': 189, 'serie': 190, 'llevado': 191, 'diversos': 192, 'soluciones': 193, 'llegan': 194, 'distar': 195, 'mucho': 196, 'entre': 197, 'sí': 198, 'e': 199, 'incluso': 200, 'contradecirse': 201, 'esta': 202, 'perspectiva': 203, 'adoptada': 204, 'real': 205, 'academia': 206, 'lengua': 207, 'española': 208, 'adoptar': 209, 'tiempoaspecto': 210, 'rae': 211, 'cap': 212, '23': 213, 'drae': 214, 'estos': 215, 'abarcan': 216, 'sólo': 217, 'sino': 218, 'también': 219, 'otra': 220, 'índole': 221, 'ejemplos': 222, 'trabajos': 223, 'boyé': 224, 'hofher': 225, '2006': 226, 'ambadiang': 227, '1994': 228, 'centramos': 229, 'formas': 230, 'analizar': 231, 'revisaremos': 232, 'basevtindpsd': 233, 'claramente': 234, 'persona': 235, 'número': 236, 'asimismo': 237, 'discutimos': 238, 'desventajas': 239, 'aproximaciones': 240, 'presentan': 241, '2a': 242, 'donde': 243, 'ba': 244, 'asignan': 245, 'pasado': 246, 'psd': 247, 'imperfecto': 248, 'imprf': 249, 'indicativo': 250, 'ind': 251, 'segmento': 252, 'n': 253, 'atribuyen': 254, 'tercera': 255, 'plural': 256, '3pl': 257, 'combinatoria': 258, 'da': 259, 'casillas': 260, 'esquemas': 261, 'etc': 262, 'bull': 263, 'parecen': 264, 'reconocer': 265, 'además': 266, 'estudio': 267, 'radicado': 268, 'determinar': 269, 'qué': 270, 'unidades': 271, 'formales': 272, 'portan': 273, 'sigue': 274, 'algunas': 275, 'realizado': 276, 'si': 277, 'opción': 278, 'tomar': 279, 'cumulativamente': 280, 'tenido': 281, 'mayor': 282, 'presencia': 283, 'parece': 284, 'existir': 285, 'todavía': 286, 'consenso': 287, 'inclinarse': 288, 'considerar': 289, 'caso': 290, 'autor': 291, 'reconoce': 292, 'pero': 293, 'específicamente': 294, 'aunque': 295, 'suponerse': 296, 'trata': 297, 'secundarias': 298, 'anteriormente': 299, 'implicacionales': 300, 'wp': 301, 'muestran': 302, 'ventajas': 303, 'metodológicas': 304, 'unidad': 305, 'mínima': 306, 'típico': 307, 'enfocamos': 308, 'varios': 309, 'aspectos': 310, 'creemos': 311, 'relevantes': 312, 'sistema': 313, 'flexivo': 314, 'debe': 315, 'partirse': 316, 'estricto': 317, 'olvidar': 318, 'problemas': 319, 'ofrece': 320, 'significación': 321, 'consideramos': 322, 'pesar': 323, 'haber': 324, 'abordada': 325, 'literatura': 326, 'ante': 327, 'otras': 328, 'perspectivas': 329, 'continuación': 330, 'comparando': 331, 'tradicionales': 332, 'aquí': 333, 'presentamos': 334, 'presentado': 335, 'buena': 336, 'cantidad': 337, 'buscan': 338, 'definir': 339, 'valera': 340, '1992': 341, 'teóricas': 342, 'tipo': 343, 'ip': 344, 'realizacionales': 345, 'buscado': 346, 'opciones': 347, 'morfologìa': 348, 'muchas': 349, 'veces': 350, 'arrastrado': 351, 'mismos': 352, 'equivalentes': 353, 'abordamos': 354, 'aquellas': 355, 'tienen': 356, 'ver': 357, 'definición': 358, 'b': 359, 'c': 360, 'morfosintácticos': 361, 'd': 362, 'irregularidades': 363, 'f': 364, 'clases': 365, 'flexivas': 366, 'conjugaciones': 367, 'amaba': 368, 'ind3pl': 369, 'elección': 370, 'basa': 371, 'hecho': 372, 'diferente': 373, 'avances': 374, 'basadas': 375, 'mostramos': 376, 'través': 377, 'evita': 378, 'búsqueda': 379, 'exhaustiva': 380, 'patrones': 381, 'necesariamente': 382, 'forman': 383, 'parte': 384, 'realidad': 385, 'lingüística': 386, 'completa': 387, 'ninguna': 388, 'menor': 389, 'contiene': 390, 'ni': 391, 'características': 392})\n"
     ]
    }
   ],
   "source": [
    "#Llamamos la funcion para crear el vocabulario\n",
    "voc = vocab()\n",
    "#Creamos el vocabulario y le asignamos un indice a cada simbolo segun su aparicion\n",
    "cads_idx = list(text2numba(train_data,voc))\n",
    "\n",
    "print(cads_idx[:3])\n",
    "print(voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IFBmY3yTW1jn"
   },
   "source": [
    "Para completar el vocabulario, agregaremos los símbolos de BOS (Beginning Of String) y EOS (End Of String). Asimismo, añadiremos estos símbolos acada cadena en el entrenamiento, de tal forma que las cadenas sean de la forma:\n",
    "\n",
    "$$<BOS> w_1 ... w_k <EOS>$$\n",
    "\n",
    "De esta forma, podremos obtener probabilidades inciales y transiciones terminales (aquellas que van hacía el símbolo de termino EOS).\n",
    "\n",
    "Asimismo, obtenemos el diccionario que nos servirá para recuperar las palabras dado los índices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "TUCVBu3b_rNj",
    "outputId": "efd9f53d-2fba-4661-d57b-0ae2195ab4aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[394, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1, 17, 18, 19, 20, 21, 22, 1, 23, 24, 18, 25, 26, 27, 1, 28, 3, 18, 1, 29, 393], [394, 30, 31, 32, 33, 34, 35, 36, 37, 1, 28, 38, 18, 1, 29, 22, 39, 40, 41, 37, 30, 42, 18, 1, 43, 18, 6, 44, 13, 45, 18, 46, 47, 48, 28, 49, 27, 50, 51, 52, 18, 53, 54, 393], [394, 55, 393]]\n",
      "70\n",
      "393 394\n"
     ]
    }
   ],
   "source": [
    "#Indicamos las etiquetas a usar\n",
    "EOS = '<EOS>'\n",
    "BOS = '<BOS>'\n",
    "\n",
    "#Cada etiqeuta se le asigna un indice numerico\n",
    "BOS_IDX = max(voc.values())+2\n",
    "EOS_IDX = max(voc.values())+1\n",
    "\n",
    "#Se agregan estas etiqeutas al vocabulario\n",
    "voc[EOS] = EOS_IDX\n",
    "voc[BOS] = BOS_IDX\n",
    "\n",
    "#A cada cadena se le agrega la etiqueta BOS al inicio y EOS al final\n",
    "cadenas = [[BOS_IDX] + cad + [EOS_IDX] for cad in cads_idx]\n",
    "\n",
    "#Diccionario de índice : palabra\n",
    "words = {idx: w for w,idx in voc.items()}\n",
    "\n",
    "print(cadenas[:3])\n",
    "print(len(cadenas))\n",
    "print(EOS_IDX, BOS_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dwsAGwopXYVR"
   },
   "source": [
    "### Estimación del modelo de n-gramas\n",
    "\n",
    "Una vez preprocesadas las cadenas podemos pasar a estimar el modelo. Para esta estimación, tomaremos en cuenta dos parámetros:\n",
    "\n",
    "\n",
    "\n",
    "*   El tamaño de n-gramas; es decir, qué tantos elementos previos consideraremos para estimar la probabilidad de que ocurra una palabra.\n",
    "*   El elemento $\\lambda$ para estimar la probabilidad con smoothing de Lidstone. En este sentido, dado un n-grama $w_{i-n+1} ... w_{i-1} w_i$ estimaremos la probabilidad como:\n",
    "\n",
    "$$p(w_i|w_{i-1}...w_{i-n+1}) = \\frac{fr(w_{i-n+1} ... w_{i-1} w_i) + \\lambda}{fr(w_{i-n+1} ... w_{i-1}) + \\lambda N}$$\n",
    "\n",
    "donde $N$ es el tamaño del vocabulario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MT-V04pf_rNv"
   },
   "outputs": [],
   "source": [
    "def get_model(cadenas, n=2, l=1.0):\n",
    "    #Se crean los bigramas\n",
    "    bigrams = chain(*[ zip(*[cad[i:] for i in range(n)] ) for cad in cadenas] )\n",
    "\n",
    "    #Se obtiene la frecuencia de cada bigrama\n",
    "    frec_grams = Counter(bigrams)\n",
    "    \n",
    "    #Obtenemos el tamaño del vocabulario (menos EOS y BOS)\n",
    "    N = len(voc) -2\n",
    "    #Determinamos las dimensiones del tensor de transciones\n",
    "    #En la palabra condicionada consideraremos al elemento EOS\n",
    "    dim = (N,)*(n-1) + (N+1,)\n",
    "\n",
    "    #Tensor de transiciones\n",
    "    A = np.zeros(dim)\n",
    "    #Probabilidades iniciales\n",
    "    Pi = np.zeros(N)\n",
    "    \n",
    "    #Frecuencias\n",
    "    for bigram,frec in frec_grams.items():\n",
    "      #Se llena el tensor de transiciones con frecuencias\n",
    "      if bigram[0] != BOS_IDX:\n",
    "          A[bigram] = frec\n",
    "      #Se onbtienen las frecuencias de iniciales\n",
    "      elif bigram[0] == BOS_IDX and bigram[1] != EOS_IDX:\n",
    "          Pi[bigram[1]] = frec\n",
    "          \n",
    "    #A partir de las frecuencias, se obtienen las probabilidades\n",
    "    #Se considera un parámetro l de smoothing de Lidstone\n",
    "    for h,b in enumerate(A):\n",
    "      A[h] = ((b+l).T/(b+l).sum(n-2)).T\n",
    "    \n",
    "    #Probabilidad de iniciales  \n",
    "    Pi = (Pi+l)/(Pi+l).sum(0)\n",
    "    \n",
    "    return A, Pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R2gvdddxaEXI"
   },
   "source": [
    "En primer lugar, podemos estimar un modelo de trigramas con $\\lambda =1$, es decir, con smoothing Laplaciano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "xRE41zXz_rN4",
    "outputId": "ac1b01e9-b44c-46d5-f720-dc6c884692ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(393, 393, 394)\n",
      "[[1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n",
      "CPU times: user 385 ms, sys: 247 ms, total: 632 ms\n",
      "Wall time: 642 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Modelo de trigramas\n",
    "mu_3g_lap = get_model(cadenas, n=3, l=1.0)\n",
    "\n",
    "#Tamaño del tensor de transicion\n",
    "print(mu_3g_lap[0].shape)\n",
    "#Suma de probabilidades\n",
    "print(mu_3g_lap[0].sum(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VJ9yBDWEaqBv"
   },
   "source": [
    "De igual forma, podemos obtener un modelo de bigramas también con smoothing Laplaciano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "OrRPdWxtaNzU",
    "outputId": "5ebfea6c-53b8-458d-ceeb-1bdec5e2980e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(393, 394)\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "CPU times: user 20.7 ms, sys: 340 µs, total: 21 ms\n",
      "Wall time: 17.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Modelo de bigramas\n",
    "mu_2g_lap = get_model(cadenas, n=2, l=1.0)\n",
    "\n",
    "#Tamaño del tensor de transicion\n",
    "print(mu_2g_lap[0].shape)\n",
    "#Suma de probabilidades\n",
    "print(mu_2g_lap[0].sum(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Q6ijoT2bDD5"
   },
   "source": [
    "### Aplicaciones de los modelos\n",
    "\n",
    "Una vez estimados estos modelos del lenguaje, podemos ver sus diferentes aplicaciones para algunas de las tareas típicas: determinar la probabilidad de una cadena, predecir la palabra siguiente y generar texto.\n",
    "\n",
    "Para determinar la probabilidad, utilizaremos la función:\n",
    "\n",
    "$$p(w_1 ... w_k) = \\prod_{i=1}^k p(w_i|w_{i-1} ... w_{i-n+1})$$\n",
    "\n",
    "Dado que las cadenas pueden extenderse y las probabilidades son pequeñas, es posible que la probabilidad se haga tan pequeña que aparezca como un cero. Para evitar esto, utilizaremos probabilidad logarítimicada, dada por:\n",
    "\n",
    "$$\\log p(w_1 ... w_k) = \\sum_{i=1}^k \\log p(w_i|w_{i-1} ... w_{i-n+1})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_fablN9k_rPo"
   },
   "outputs": [],
   "source": [
    "#Probabilidad logarítimica\n",
    "def prob_str(string, mu):\n",
    "    #Elementos del modelo\n",
    "    A, Pi = mu\n",
    "    #n-gramas\n",
    "    n = len(A.shape)\n",
    "    #Palabras de la cadena\n",
    "    sent = [voc[w] for w in string.split()]\n",
    "    \n",
    "    #Probabilidad inicial\n",
    "    try:\n",
    "      p = np.log(Pi[voc[sent[0]]])\n",
    "    #OOV\n",
    "    except:\n",
    "      p = 0.0\n",
    "    \n",
    "    #Obtener n-gramas de la cadena\n",
    "    grams = list(zip(*[sent[i:] for i in range(n)]))\n",
    "    #Probabilidad de n-gramas\n",
    "    for gram in grams:\n",
    "        try:\n",
    "          p += np.log(A[gram])\n",
    "        #OOV\n",
    "        except:\n",
    "          p += 0.0\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TmIkc-oRcMUD"
   },
   "source": [
    "Podemos probar esta estimación sobre cadenas. Para obtener la probabilidad exacta, sólo basta elevar a la exponencial la probabilidad logarítmica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "lcWmYt6z_rP2",
    "outputId": "d789b3dd-a4f7-4112-eb10-c3fa3ce8f39b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidad para la cadena: \tacerca la flexión verbal del español los enfoques clásicos han dado gran peso a una discusión sobre la precisión de diferentes lindes segmentales y la asignación pertinente de rasgos esto es la estructura verbal de la palabra\n",
      "\t Modelo trigramas: 1.0\n",
      "\t Modelo bigramas: 1.0\n"
     ]
    }
   ],
   "source": [
    "print('Probabilidad para la cadena:', train_data[0])\n",
    "print('\\t Modelo trigramas:', np.exp(prob_str(train_data[5], mu_3g_lap)))\n",
    "print('\\t Modelo bigramas:', np.exp(prob_str(train_data[5], mu_2g_lap)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mi1he_J7c2WL"
   },
   "source": [
    "Otra de las aplicaciones es la predicción de la palabra subsiguiente; para esto buscaremos:\n",
    "  \n",
    "$$\\arg\\max_w p(w_1 ... w_k w)$$\n",
    "\n",
    "Es claro que la probabilidad logarítimica no afecta el resultado, es decir, se cumple la igualdad:\n",
    "\n",
    "$$\\arg\\max_w p(w_1 ... w_k w) = \\arg\\max_w \\log p(w_1 ... w_k w)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KUvvIXLi_rP_"
   },
   "outputs": [],
   "source": [
    "def next_word(string, mu):\n",
    "    #n-grams\n",
    "    history = len(mu[0].shape)-1\n",
    "    #Palabras en la cadena\n",
    "    sent = [voc[w] for w in string.split()]\n",
    "    #Los elementos previos\n",
    "    prev_gram = tuple(sent[-history:])\n",
    "    #Se obtiene probabilidad d ela cadena previa\n",
    "    p = prob_str(string, mu)\n",
    "    #Se obtienen las probabilidades de los elementps subsiguientes\n",
    "    #Se obtiene el máximo de estas probabilidades\n",
    "    next = np.argmax(p + np.log(mu[0][prev_gram]))\n",
    "    \n",
    "    return words[next]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "kqWHlwQe_rQM",
    "outputId": "24bf6cf5-ea22-474f-ab53-6b1fcd0227d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "muestra\n",
      "<EOS>\n"
     ]
    }
   ],
   "source": [
    "print(next_word('en el español', mu_3g_lap))\n",
    "print(next_word('en el español', mu_2g_lap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3peUXkthiVdm"
   },
   "source": [
    "Finalmente, podemos iterar la función anterior para producir una función de generación del lenguaje. El algoritmo se detendra cuando encuentre el símbolo EOS, o despúes de producir 100 palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vU9H5YM6_rQb"
   },
   "outputs": [],
   "source": [
    "def generate(string, mu):\n",
    "    #Inicialización palabra\n",
    "    w = ''\n",
    "    #Inicialización cadena generada\n",
    "    str_gen = string\n",
    "    \n",
    "    i = 0\n",
    "    #Mientras no se prediga EOS\n",
    "    while w != '<EOS>':\n",
    "        #Obtener la palabra subsiguiente\n",
    "        w = next_word(str_gen, mu)\n",
    "        #Se añade a la cadena generada\n",
    "        str_gen += ' ' + w\n",
    "        \n",
    "        i += 1\n",
    "        #Se deteiene el algoritmo si ha alcanzado 100 iteraciones\n",
    "        if i == 100:\n",
    "            break\n",
    "        \n",
    "    return str_gen\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "NNJY8Xfd_rQo",
    "outputId": "e3aa1cc3-070f-4e03-b0d7-9323d25b8961"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo trigramas: el español muestra una combinación de ta a este morfema dejando de lado el modo o bien atribuyéndose a la palabra verbal del español <EOS>\n",
      "Modelo bigramas: el español <EOS>\n"
     ]
    }
   ],
   "source": [
    "print('Modelo trigramas:', generate('el español', mu_3g_lap))\n",
    "print('Modelo bigramas:', generate('el español', mu_2g_lap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gxvdB7ggi-wn"
   },
   "source": [
    "### Evaluación de los modelos\n",
    "\n",
    "Finalmente, podemos evaluar los modelos a partir de su entropía. Esta se determina en un corpus de evaluación, que nunca debió ser visto por el entrenamiento. Calcularemos la entropía como:\n",
    "\n",
    "$$H(p) = -\\frac{1}{K} \\sum_{i=1}^k p(w_1 ... w_k)$$\n",
    "\n",
    "A partir de la entropía podríamos caluclar la perplejidad como $2^{H(p)}$. (Debe notarse que, como estamos estimando probabilidades logarítmicas, no hará falta obtener un logaritmo en esta función)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XQVeJLVZ_rQ0"
   },
   "outputs": [],
   "source": [
    "def get_entropy(mu):\n",
    "  #Inicialización Entropía\n",
    "  H = 0.0\n",
    "  #Evaluamos en el corpus de evaluación\n",
    "  for cad in test_data:\n",
    "      #Probabilidad de la cadena\n",
    "      logp_cad = prob_str(cad, mu)\n",
    "      #Número de palabras\n",
    "      M = len(cad.split())\n",
    "      #Obtenemos la entropía cruzada de la cadena\n",
    "      H -= logp_cad/(M+1e-100)       \n",
    "\n",
    "  return H/len(test_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "fDK_dBSB_rRB",
    "outputId": "220ad048-4bbc-4c45-fe63-cc09633ee6a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Entropía--\n",
      "\t Modelo de bigramas: 3.5923932771091\n",
      "\t Modelo de trigramas: 2.7408636864829425\n"
     ]
    }
   ],
   "source": [
    "print('--Entropía--')\n",
    "print('\\t Modelo de bigramas:', 2**get_entropy(mu_2g_lap))\n",
    "print('\\t Modelo de trigramas:', 2**get_entropy(mu_3g_lap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "olFjdLxlkOJ_"
   },
   "source": [
    "En general, los modelos de trigramas muestran una entropía más baja (o perplejidad más baja) (Jurafsky y Martin, 2018). También podemos estimar cuál es el mejor parámetro en la probabilidad de Lidstone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "_MD0fW1yR_rN",
    "outputId": "31e37ad5-165e-4412-a427-ea836388c761"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 46s, sys: 3min 41s, total: 6min 27s\n",
      "Wall time: 6min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trig_entropy = []\n",
    "big_entropy = []\n",
    "\n",
    "for k in range(1,200):\n",
    "  #Parámetro\n",
    "  par = k/100\n",
    "  #Entropía del modelo de 2-gramas\n",
    "  ent2 = get_entropy(get_model(cadenas, n=2, l=par))\n",
    "  #Entropía del modelo 3-gramas\n",
    "  ent3 = get_entropy(get_model(cadenas, n=3, l=par))\n",
    "  \n",
    "  trig_entropy.append([par, ent3])\n",
    "  big_entropy.append([par, ent2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pEFj7VSoksBc"
   },
   "source": [
    "Visualizamos cómo se comporta la entropía con respecto al parámetro $\\lambda$ en un modelo de trigramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "nfmzIjUaWSGq",
    "outputId": "bb6ab488-fef0-414b-83e4-f6d1518c9840"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trig_entropy = np.array(trig_entropy)\n",
    "big_entropy = np.array(big_entropy)\n",
    "\n",
    "#Visualización del modelo\n",
    "plt.scatter(trig_entropy[:,0], trig_entropy[:,1], color='r', s=4, label='Trigram model')\n",
    "plt.scatter(big_entropy[:,0], big_entropy[:,1], color='g', s=4, label='Bigram model')\n",
    "plt.legend(bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jqC6U0UIk54h"
   },
   "source": [
    "Y podemos utilizar este parámetro para estimar un modelo de trigramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "LBaZs6qBk416",
    "outputId": "3267f686-d4af-4e59-f277-b7ded28b5249"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda óptimo: 1.12\n",
      "Probabilidad de la cadena: \t\t\tbasevtpsd\n",
      "\t 0.0018214936247723135\n"
     ]
    }
   ],
   "source": [
    "#Parámetro óptimo\n",
    "lamb = trig_entropy[:,0][np.argmin(trig_entropy[:,1])]\n",
    "print('Lambda óptimo:', lamb)\n",
    "\n",
    "#Estimación del modelo\n",
    "mu_3g_lid = get_model(cadenas, n=3, l=lamb)\n",
    "\n",
    "#Ejemplo de probabilidad\n",
    "print('Probabilidad de la cadena:', train_data[5])\n",
    "print('\\t', np.exp(prob_str(train_data[5], mu_3g_lid)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copia de N-grams.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
